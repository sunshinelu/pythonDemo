{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://www.52nlp.cn/tag/deep-learning\n",
    "# http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = [line.strip() for line in file('nlpDemo/coursera_corpus')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Writing II: Rhetorical Composing', 'Genetics and Society: A Course for Educators', 'General Game Playing', 'Genes and the Human Condition (From Behavior to Biotechnology)', 'A Brief History of Humankind', 'New Models of Business in Society', 'Analyse Num\\xc3\\xa9rique pour Ing\\xc3\\xa9nieurs', 'Evolution: A Course for Educators', 'Coding the Matrix: Linear Algebra through Computer Science Applications', 'The Dynamic Earth: A Course for Educators']\n"
     ]
    }
   ],
   "source": [
    "courses_name = [course.split('\\t')[0] for course in courses]\n",
    "print courses_name[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'BROWN CORPUS\\n\\nA Standard Corpus of Present-Day Edited American\\nEnglish, for use with Digital Computers.\\n\\nby W. N. Francis and H. Kucera (1964)\\nDepartment of Linguistics, Brown University\\nProvidence, Rhode Island, USA\\n\\nRevised 1971, Revised and Amplified 1979\\n\\nhttp://www.hit.uib.no/icame/brown/bcm.html\\n\\nDistributed with the permission of the copyright holder,\\nredistribution permitted.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'The', u'AT'),\n (u'Fulton', u'NP-TL'),\n (u'County', u'NN-TL'),\n (u'Grand', u'JJ-TL'),\n (u'Jury', u'NN-TL'),\n (u'said', u'VBD'),\n (u'Friday', u'NR'),\n (u'an', u'AT'),\n (u'investigation', u'NN'),\n (u'of', u'IN')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['writing', 'ii:', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'you', 'in', 'a', 'series', 'of', 'interactive', 'reading,', 'research,', 'and', 'composing', 'activities', 'along', 'with', 'assignments', 'designed', 'to', 'help', 'you', 'become', 'more', 'effective', 'consumers', 'and', 'producers', 'of', 'alphabetic,', 'visual', 'and', 'multimodal', 'texts.', 'join', 'us', 'to', 'become', 'more', 'effective', 'writers...', 'and', 'better', 'citizens.', 'rhetorical', 'composing', 'is', 'a', 'course', 'where', 'writers', 'exchange', 'words,', 'ideas,', 'talents,', 'and', 'support.', 'you', 'will', 'be', 'introduced', 'to', 'a', 'variety', 'of', 'rhetorical', 'concepts\\xe2\\x80\\x94that', 'is,', 'ideas', 'and', 'techniques', 'to', 'inform', 'and', 'persuade', 'audiences\\xe2\\x80\\x94that', 'will', 'help', 'you', 'become', 'a', 'more', 'effective', 'consumer', 'and', 'producer', 'of', 'written,', 'visual,', 'and', 'multimodal', 'texts.', 'the', 'class', 'includes', 'short', 'videos,', 'demonstrations,', 'and', 'activities.', 'we', 'envision', 'rhetorical', 'composing', 'as', 'a', 'learning', 'community', 'that', 'includes', 'both', 'those', 'enrolled', 'in', 'this', 'course', 'and', 'the', 'instructors.', 'we', 'bring', 'our', 'expertise', 'in', 'writing,', 'rhetoric', 'and', 'course', 'design,', 'and', 'we', 'have', 'designed', 'the', 'assignments', 'and', 'course', 'infrastructure', 'to', 'help', 'you', 'share', 'your', 'experiences', 'as', 'writers,', 'students,', 'and', 'professionals', 'with', 'each', 'other', 'and', 'with', 'us.', 'these', 'collaborations', 'are', 'facilitated', 'through', 'wex,', 'the', 'writers', 'exchange,', 'a', 'place', 'where', 'you', 'will', 'exchange', 'your', 'work', 'and', 'feedback']\n"
     ]
    }
   ],
   "source": [
    "texts_lower = [[word for word in document.lower().split()] for document in courses]\n",
    "print texts_lower[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'writing', u'ii', u':', u'rhetorical', u'composing', u'rhetorical', u'composing', u'engages', u'you', u'in', u'a', u'series', u'of', u'interactive', u'reading', u',', u'research', u',', u'and', u'composing', u'activities', u'along', u'with', u'assignments', u'designed', u'to', u'help', u'you', u'become', u'more', u'effective', u'consumers', u'and', u'producers', u'of', u'alphabetic', u',', u'visual', u'and', u'multimodal', u'texts', u'.', u'join', u'us', u'to', u'become', u'more', u'effective', u'writers', u'...', u'and', u'better', u'citizens', u'.', u'rhetorical', u'composing', u'is', u'a', u'course', u'where', u'writers', u'exchange', u'words', u',', u'ideas', u',', u'talents', u',', u'and', u'support', u'.', u'you', u'will', u'be', u'introduced', u'to', u'a', u'variety', u'of', u'rhetorical', u'concepts\\u2014that', u'is', u',', u'ideas', u'and', u'techniques', u'to', u'inform', u'and', u'persuade', u'audiences\\u2014that', u'will', u'help', u'you', u'become', u'a', u'more', u'effective', u'consumer', u'and', u'producer', u'of', u'written', u',', u'visual', u',', u'and', u'multimodal', u'texts', u'.', u'the', u'class', u'includes', u'short', u'videos', u',', u'demonstrations', u',', u'and', u'activities', u'.', u'we', u'envision', u'rhetorical', u'composing', u'as', u'a', u'learning', u'community', u'that', u'includes', u'both', u'those', u'enrolled', u'in', u'this', u'course', u'and', u'the', u'instructors', u'.', u'we', u'bring', u'our', u'expertise', u'in', u'writing', u',', u'rhetoric', u'and', u'course', u'design', u',', u'and', u'we', u'have', u'designed', u'the', u'assignments', u'and', u'course', u'infrastructure', u'to', u'help', u'you', u'share', u'your', u'experiences', u'as', u'writers', u',', u'students', u',', u'and', u'professionals', u'with', u'each', u'other', u'and', u'with', u'us', u'.', u'these', u'collaborations', u'are', u'facilitated', u'through', u'wex', u',', u'the', u'writers', u'exchange', u',', u'a', u'place', u'where', u'you', u'will', u'exchange', u'your', u'work', u'and', u'feedback']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "texts_tokenized = [[word.lower() for word in word_tokenize(document.decode('utf-8'))] for document in courses]\n",
    "print texts_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "print english_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'writing', u'ii', u':', u'rhetorical', u'composing', u'rhetorical', u'composing', u'engages', u'series', u'interactive', u'reading', u',', u'research', u',', u'composing', u'activities', u'along', u'assignments', u'designed', u'help', u'become', u'effective', u'consumers', u'producers', u'alphabetic', u',', u'visual', u'multimodal', u'texts', u'.', u'join', u'us', u'become', u'effective', u'writers', u'...', u'better', u'citizens', u'.', u'rhetorical', u'composing', u'course', u'writers', u'exchange', u'words', u',', u'ideas', u',', u'talents', u',', u'support', u'.', u'introduced', u'variety', u'rhetorical', u'concepts\\u2014that', u',', u'ideas', u'techniques', u'inform', u'persuade', u'audiences\\u2014that', u'help', u'become', u'effective', u'consumer', u'producer', u'written', u',', u'visual', u',', u'multimodal', u'texts', u'.', u'class', u'includes', u'short', u'videos', u',', u'demonstrations', u',', u'activities', u'.', u'envision', u'rhetorical', u'composing', u'learning', u'community', u'includes', u'enrolled', u'course', u'instructors', u'.', u'bring', u'expertise', u'writing', u',', u'rhetoric', u'course', u'design', u',', u'designed', u'assignments', u'course', u'infrastructure', u'help', u'share', u'experiences', u'writers', u',', u'students', u',', u'professionals', u'us', u'.', u'collaborations', u'facilitated', u'wex', u',', u'writers', u'exchange', u',', u'place', u'exchange', u'work', u'feedback']\n"
     ]
    }
   ],
   "source": [
    "texts_filtered_stopwords = [[word for word in document if not word in english_stopwords] for document in texts_tokenized]\n",
    "print texts_filtered_stopwords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'writing', u'ii', u'rhetorical', u'composing', u'rhetorical', u'composing', u'engages', u'series', u'interactive', u'reading', u'research', u'composing', u'activities', u'along', u'assignments', u'designed', u'help', u'become', u'effective', u'consumers', u'producers', u'alphabetic', u'visual', u'multimodal', u'texts', u'join', u'us', u'become', u'effective', u'writers', u'...', u'better', u'citizens', u'rhetorical', u'composing', u'course', u'writers', u'exchange', u'words', u'ideas', u'talents', u'support', u'introduced', u'variety', u'rhetorical', u'concepts\\u2014that', u'ideas', u'techniques', u'inform', u'persuade', u'audiences\\u2014that', u'help', u'become', u'effective', u'consumer', u'producer', u'written', u'visual', u'multimodal', u'texts', u'class', u'includes', u'short', u'videos', u'demonstrations', u'activities', u'envision', u'rhetorical', u'composing', u'learning', u'community', u'includes', u'enrolled', u'course', u'instructors', u'bring', u'expertise', u'writing', u'rhetoric', u'course', u'design', u'designed', u'assignments', u'course', u'infrastructure', u'help', u'share', u'experiences', u'writers', u'students', u'professionals', u'us', u'collaborations', u'facilitated', u'wex', u'writers', u'exchange', u'place', u'exchange', u'work', u'feedback']\n"
     ]
    }
   ],
   "source": [
    "texts_filtered = [[word for word in document if not word in english_punctuations] for document in texts_filtered_stopwords]\n",
    "print texts_filtered[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stem'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "st.stem('stemmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stem'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('stemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stem'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('stemmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maxim'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('presumably')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'writ', u'ii', u'rhet', u'compos', u'rhet', u'compos', u'eng', u'sery', u'interact', u'read', u'research', u'compos', u'act', u'along', u'assign', u'design', u'help', u'becom', u'effect', u'consum', u'produc', u'alphabet', u'vis', u'multimod', u'text', u'join', u'us', u'becom', u'effect', u'writ', u'...', u'bet', u'cit', u'rhet', u'compos', u'cours', u'writ', u'exchang', u'word', u'idea', u'tal', u'support', u'introduc', u'vary', u'rhet', u'concepts\\u2014that', u'idea', u'techn', u'inform', u'persuad', u'audiences\\u2014that', u'help', u'becom', u'effect', u'consum', u'produc', u'writ', u'vis', u'multimod', u'text', u'class', u'includ', u'short', u'video', u'demonst', u'act', u'envid', u'rhet', u'compos', u'learn', u'commun', u'includ', u'enrol', u'cours', u'instruct', u'bring', u'expert', u'writ', u'rhet', u'cours', u'design', u'design', u'assign', u'cours', u'infrastruct', u'help', u'shar', u'expery', u'writ', u'stud', u'profess', u'us', u'collab', u'facilit', u'wex', u'writ', u'exchang', u'plac', u'exchang', u'work', u'feedback']\n"
     ]
    }
   ],
   "source": [
    "texts_stemmed = [[st.stem(word) for word in docment] for docment in texts_filtered]\n",
    "print texts_stemmed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stems = sum(texts_stemmed, [])\n",
    "stems_once = set(stem for stem in set(all_stems) if all_stems.count(stem) == 1)\n",
    "texts = [[stem for stem in text if stem not in stems_once] for text in texts_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:19:34,459 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:19:34,557 : INFO : built Dictionary(2974 unique tokens: [u\"d'ex\\xe9cution\", u'four', u'protest', u'circuitry', u'proficy']...) from 379 documents (total 47550 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:19:48,598 : INFO : collecting document frequencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:19:48,599 : INFO : PROGRESS: processing document #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:19:48,623 : INFO : calculating IDF weights for 379 documents and 2973 features (28933 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:06,685 : INFO : using serial LSI version on this node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:06,690 : INFO : updating model with new documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:06,903 : INFO : preparing a new chunk of documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:06,915 : INFO : using 100 extra samples and 2 power iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:06,916 : INFO : 1st phase: constructing (2974, 110) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:06,937 : INFO : orthonormalizing (2974, 110) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:07,080 : INFO : 2nd phase: running dense svd on (110, 379) matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:07,106 : INFO : computing the final decomposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:07,108 : INFO : keeping 10 factors (discarding 72.559% of energy spectrum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:07,114 : INFO : processed documents up to #379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:07,119 : INFO : topic #0(3.823): 0.238*\"teach\" + 0.152*\"nbsp\" + 0.136*\"program\" + 0.118*\"learn\" + 0.115*\"mus\" + 0.106*\"heal\" + 0.103*\"comput\" + 0.098*\"stud\" + 0.098*\"network\" + 0.093*\"design\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:07,122 : INFO : topic #1(2.703): 0.611*\"de\" + 0.380*\"la\" + 0.217*\"en\" + 0.166*\"Ã \" + 0.162*\"los\" + 0.151*\"que\" + 0.149*\"cour\" + 0.144*\"un\" + 0.132*\"les\" + 0.120*\"curso\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:07,126 : INFO : topic #2(2.507): -0.619*\"teach\" + -0.206*\"portfolio\" + -0.191*\"assist\" + 0.150*\"mus\" + -0.133*\"improv\" + -0.118*\"undertak\" + -0.117*\"learn\" + -0.116*\"strongly\" + 0.112*\"network\" + -0.111*\"found\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:07,129 : INFO : topic #3(2.338): 0.808*\"mus\" + -0.173*\"heal\" + 0.170*\"sound\" + 0.097*\"art\" + 0.095*\"audio\" + 0.091*\"digit\" + -0.082*\"glob\" + -0.080*\"econom\" + 0.077*\"program\" + 0.072*\"composit\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:07,140 : INFO : topic #4(2.208): 0.358*\"heal\" + -0.292*\"program\" + 0.223*\"mus\" + -0.221*\"comput\" + -0.193*\"algorithm\" + -0.189*\"dat\" + -0.180*\"langu\" + 0.167*\"glob\" + -0.150*\"network\" + 0.123*\"food\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:07,154 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 17:20:07,270 : INFO : creating matrix with 379 documents and 10 features\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\n",
    "\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning\n"
     ]
    }
   ],
   "source": [
    "print courses_name[210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 8.6166020318676892), (1, -0.72098372703180857), (2, -0.81795987998954789), (3, 0.27369270757071357), (4, -4.7009538912646613), (5, 0.88853722469178931), (6, 1.8913865379426358), (7, -1.4958198510709599), (8, 0.28634408545247597), (9, 0.79435321408493653)]\n"
     ]
    }
   ],
   "source": [
    "ml_course = texts[210]\n",
    "ml_bow = dictionary.doc2bow(ml_course)\n",
    "ml_lsi = lsi[ml_bow]\n",
    "print ml_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[ml_lsi]\n",
    "sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(210, 0.99999994), (174, 0.97528434), (141, 0.95233381), (63, 0.95154613), (189, 0.95100498), (238, 0.9418624), (184, 0.94065017), (220, 0.93756258), (111, 0.93348938), (221, 0.93343323)]\n"
     ]
    }
   ],
   "source": [
    "print sort_sims[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning\n"
     ]
    }
   ],
   "source": [
    "print courses_name[210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning\n"
     ]
    }
   ],
   "source": [
    "print courses_name[174]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilistic Graphical Models\n"
     ]
    }
   ],
   "source": [
    "print courses_name[238]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Networks for Machine Learning\n"
     ]
    }
   ],
   "source": [
    "print courses_name[203]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
